{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn import preprocessing\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#settimg path for the directory\n",
    "import os\n",
    "os.chdir(\"C:/Users/Lalith/Downloads/P1 Data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#loding the data sets from the directory\n",
    "filepath1='C:/Users/Lalith/Downloads/P1 Data/Consumer_Complaints_train.csv'\n",
    "pm15_train=pd.read_csv(filepath1)\n",
    "filepath2='C:/Users/Lalith/Downloads/P1 Data/Consumer_Complaints_test_share.csv'\n",
    "pm15_test=pd.read_csv(filepath2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#renaming the column name\n",
    "pm15_train=pm15_train.rename(columns={'Consumer disputed?':'consumerdisputed'})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#printing the head for train dataset\n",
    "pm15_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#printing the head for the test data set\n",
    "pm15_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "ll=LabelEncoder()\n",
    "pm15_train['Company']=ll.fit_transform(pm15_train['Company'])\n",
    "pm15_test['Company']=ll.fit_transform(pm15_test['Company'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pm15_train['Issue']=ll.fit_transform(pm15_train['Issue'])\n",
    "pm15_test['Issue']=ll.fit_transform(pm15_test['Issue'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pm15_train['Sub-issue']=pm15_train['Sub-issue'].str.replace(\" \",\"\")\n",
    "pm15_train['Sub-issue'] = pm15_train['Sub-issue'].fillna(pm15_train['Sub-issue'].mode()[0])\n",
    "pm15_train['Sub-issue']=ll.fit_transform(pm15_train['Sub-issue'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pm15_test['Sub-issue']=pm15_test['Sub-issue'].str.replace(\" \",\"\")\n",
    "pm15_test['Sub-issue'] = pm15_test['Sub-issue'].fillna(pm15_test['Sub-issue'].mode()[0])\n",
    "pm15_test['Sub-issue']=ll.fit_transform(pm15_test['Sub-issue'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pm15_train['Sub-product']=pm15_train['Sub-product'].str.replace(\" \",\"\")\n",
    "pm15_train['Sub-product'] = pm15_train['Sub-product'].fillna(pm15_train['Sub-product'].mode()[0])\n",
    "pm15_train['Sub-product']=ll.fit_transform(pm15_train['Sub-product'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pm15_test['Sub-product']=pm15_test['Sub-product'].str.replace(\" \",\"\")\n",
    "pm15_test['Sub-product'] = pm15_test['Sub-product'].fillna(pm15_test['Sub-product'].mode()[0])\n",
    "pm15_test['Sub-product']=ll.fit_transform(pm15_test['Sub-product'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pm15_train['Product']=pm15_train['Product'].str.replace(\" \",\"\")\n",
    "pm15_train['Product']=ll.fit_transform(pm15_train['Product'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pm15_test['Product']=pm15_test['Product'].str.replace(\" \",\"\")\n",
    "pm15_test['Product']=ll.fit_transform(pm15_test['Product'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "pm15_train['Company public response'] = pm15_train['Company public response'].fillna(pm15_train['Company public response'].mode()[0])\n",
    "pm15_test['Company public response'] = pm15_test['Company public response'].fillna(pm15_test['Company public response'].mode()[0])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pm15_train['State'] = pm15_train['State'].fillna(pm15_train['State'].mode()[0])\n",
    "pm15_train['State']=ll.fit_transform(pm15_train['State'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pm15_test['State'] = pm15_test['State'].fillna(pm15_test['State'].mode()[0])\n",
    "pm15_test['State']=ll.fit_transform(pm15_test['State'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pm15_train['Consumer consent provided?'] = pm15_train['Consumer consent provided?'].fillna(pm15_train['Consumer consent provided?'].mode()[0])\n",
    "pm15_train['Consumer consent provided?']=ll.fit_transform(pm15_train['Consumer consent provided?'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pm15_test['Consumer consent provided?'] = pm15_test['Consumer consent provided?'].fillna(pm15_test['Consumer consent provided?'].mode()[0])\n",
    "pm15_test['Consumer consent provided?']=ll.fit_transform(pm15_test['Consumer consent provided?'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pm15_train['Company response to consumer']=ll.fit_transform(pm15_train['Company response to consumer'])\n",
    "pm15_test['Company response to consumer']=ll.fit_transform(pm15_test['Company response to consumer'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#droping insignificant columns, and the column which we are going to take as target\\dependent variable\n",
    "X_train=pm15_train.drop(['Complaint ID','consumerdisputed','ZIP code'],axis=1)\n",
    "\n",
    "X_test=pm15_test.drop(['Complaint ID','ZIP code',],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#taking dependent variable and converting it's values into machine readble formet\n",
    "y_train=pm15_train['consumerdisputed'].map({'Yes':1,'No':0,})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.countplot(x=y_train,data=pm15_train, palette='hls')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#printing the training set to know the object type columns\n",
    "X_train.dtypes=='object'\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.crosstab(pm15_train['Product'],y_train).plot(kind='bar')\n",
    "plt.title('product analysis')\n",
    "plt.xlabel('product')\n",
    "plt.ylabel('consumerdisputed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#converting the the date column  which is in object type into datetype  for training data set\n",
    "X_train['Date received']=pd.to_datetime(X_train['Date received'])\n",
    "X_train['Date sent to company']=pd.to_datetime(X_train['Date sent to company'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#converting the the date column  which is in object type into datetype  for test data set\n",
    "\n",
    "X_test['Date received']=pd.to_datetime(X_test['Date received'])\n",
    "X_test['Date sent to company']=pd.to_datetime(X_test['Date sent to company'])\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we are taking the diference of the the two Date sent to company,Date received to train the model because individual date columns has no meaningfull insight for the ata set \n",
    "X_train['day_diff']=pd.to_numeric(X_train['Date sent to company']-X_train['Date received'])\n",
    "X_train.drop(['Date sent to company','Date received','State'],axis=1,inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test['day_diff']=pd.to_numeric(X_test['Date sent to company']-X_test['Date received'])\n",
    "X_test.drop(['Date sent to company','Date received','State'],axis=1,inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "X_train['Consumer complaint narrative']=pd.to_numeric(X_train['Consumer complaint narrative'],errors='coerce')\n",
    "X_train['Consumer complaint narrative']=ll.fit_transform(X_train['Consumer complaint narrative'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "X_test['Consumer complaint narrative']=pd.to_numeric(X_test['Consumer complaint narrative'],errors='coerce')\n",
    "X_test['Consumer complaint narrative']=ll.fit_transform(X_test['Consumer complaint narrative'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.crosstab(pm15_train['Company response to consumer'],y_train).plot(kind='bar')\n",
    "plt.title('company responses forthe consumer')\n",
    "plt.xlabel('Company response to consumer')\n",
    "plt.ylabel('consumerdisputed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#printing training dataset head to see the alinghment\n",
    "#X_train.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#X_test.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#spliting the traning data set into objecttype and numbertype for further processing  \n",
    "numcol=X_train.select_dtypes(include=[np.number])\n",
    "catcol=X_train.select_dtypes(include=[np.object])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#spliting the test data set into objecttype and numbertype for further processing  \n",
    "\n",
    "numcol1=X_test.select_dtypes(include=[np.number])\n",
    "catcol1=X_test.select_dtypes(include=[np.object])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#getting dummies for the obect type columns of training data set\n",
    "catcol=pd.get_dummies(catcol)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#getting dummies for the obect type columns of test data set\n",
    "\n",
    "catcol1=pd.get_dummies(catcol1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#scaling the number type columns to normalize the data of training data set\n",
    "numcol=preprocessing.scale(numcol)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#scaling the number type columns to normalize the data of test data set\n",
    "\n",
    "numcol1=preprocessing.scale(numcol1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#converting the array type numeric columns into dataframe to be concat with categorical types dataframe\n",
    "numcol=pd.DataFrame(numcol)#training data set \n",
    "numcol1=pd.DataFrame(numcol1)#test data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#concatinating the two data frames into single dataframe to be used as X_train \n",
    "X_train=pd.concat([catcol,numcol],axis=1)#we are concating with reference to columns by taking axis=1, if 0 it will do row wise concatination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#concatinating the two data frames into single dataframe to be used as X_test\n",
    "\n",
    "X_test=pd.concat([catcol1,numcol1],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing train_test_split function to split the data sets and \n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_train, y_train,random_state=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "                                                #MODEL:1\n",
    "#importing logistic regression model and fiting the model  \n",
    "from sklearn.linear_model import LogisticRegression\n",
    "clf=LogisticRegression(C=1,max_iter=4oo)\n",
    "clf.fit(X_train,y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predicting the test data  \n",
    "predictions=clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logreg=clf.predict_proba(X_test)[:,1]\n",
    "print(logreg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#printing accurecy\n",
    "from sklearn.metrics import accuracy_score,confusion_matrix\n",
    "acc=accuracy_score(y_test,predictions)\n",
    "print(acc*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import roc_curve\n",
    "logit_roc_auc = roc_auc_score(y_test, predictions)\n",
    "fpr, tpr, thresholds = roc_curve(y_test, logreg)\n",
    "plt.figure()\n",
    "plt.plot(fpr, tpr, label='Logistic Regression (area = %0.2f)' % logit_roc_auc)\n",
    "plt.plot([0, 1], [0, 1],'r--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver operating characteristic')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.savefig('Log_ROC')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions=clf.predict(X_test)\n",
    "from sklearn.metrics import accuracy_score,confusion_matrix\n",
    "cm = confusion_matrix(y_test,predictions)\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pridict_threshold (clf,X_test,threshold):\n",
    "    return np.where(logreg>threshold,1,0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "for thr in np.arange(0,.3,.1):\n",
    "    predictions=pridict_threshold(clf,X_test,thr)\n",
    "    print(confusion_matrix(y_test,predictions))   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing metrics to find accurecy\n",
    "from sklearn.metrics import accuracy_score,confusion_matrix\n",
    "cm = confusion_matrix(y_test,predictions)\n",
    "print(cm)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "AUC=roc_auc_score(y_test,predictions)\n",
    "print(AUC*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import roc_curve\n",
    "logit_roc_auc = roc_auc_score(y_test, predictions)\n",
    "fpr, tpr, thresholds = roc_curve(y_test, logreg)\n",
    "plt.figure()\n",
    "plt.plot(fpr, tpr, label='Logistic Regression (area = %0.2f)' % logit_roc_auc)\n",
    "plt.plot([0, 1], [0, 1],'r--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver operating characteristic')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.savefig('Log_ROC')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions=np.where(clf.predict(X_test)==1,\"Yes\",\"No\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "                                              #MODEL:2\n",
    "\n",
    "#importing XGBoost Classifier\n",
    "#! pip install xgboost\n",
    "from xgboost import XGBClassifier\n",
    "xgb= XGBClassifier(learning_rate=1,base_score=0.2,n_estimators=1500,max_depth=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fitting the xgboost model\n",
    "xgb.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#predicting the values on X_test\n",
    "prediction=xgb.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#printing accuracy for the xgboost model\n",
    "acc=accuracy_score(y_test,prediction)\n",
    "print(acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "probability=xgb.predict_proba(X_test)[:,1]\n",
    "\n",
    "print(probability)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for thr in np.arange(0,1,.1):\n",
    "    predictions=pridict_threshold(clf,X_test,thr)\n",
    "    print(confusion_matrix(y_test,prediction))   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score,confusion_matrix\n",
    "cm = confusion_matrix(y_test,prediction)\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "AUC=roc_auc_score(y_test,prediction)\n",
    "\n",
    "print(AUC)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
